{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division,print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils; reload(utils)\n",
    "from utils import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"data/dogscats/\"\n",
    "#path = \"data/dogscats/sample/\"\n",
    "# As large as you can, but no larger than 64 is recommended. \n",
    "# If you have an older or cheaper GPU, you'll run out of memory, so will have to decrease this.\n",
    "batch_size=90\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg = Vgg16()\n",
    "# Grab a few images at a time for training and validation.\n",
    "# NB: They must be in subdirectories named based on their category\n",
    "batches = vgg.get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = vgg.get_batches(path+'valid', batch_size=batch_size*2)\n",
    "batches.class_indices\n",
    "val_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 620s - loss: 0.1285 - acc: 0.9656 - val_loss: 0.0559 - val_acc: 0.9830\n"
     ]
    }
   ],
   "source": [
    "vgg.finetune(batches)\n",
    "vgg.fit(batches, val_batches, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "23000/23000 [==============================] - 623s - loss: 0.3319 - acc: 0.9119 - val_loss: 0.0889 - val_acc: 0.9740\n",
      "Epoch 2/5\n",
      "23000/23000 [==============================] - 622s - loss: 0.2836 - acc: 0.9197 - val_loss: 0.0831 - val_acc: 0.9740\n",
      "Epoch 3/5\n",
      "23000/23000 [==============================] - 621s - loss: 0.2668 - acc: 0.9226 - val_loss: 0.0587 - val_acc: 0.9795\n",
      "Epoch 4/5\n",
      "23000/23000 [==============================] - 621s - loss: 0.2640 - acc: 0.9236 - val_loss: 0.0598 - val_acc: 0.9795\n",
      "Epoch 5/5\n",
      "23000/23000 [==============================] - 622s - loss: 0.2765 - acc: 0.9198 - val_loss: 0.0767 - val_acc: 0.9815\n"
     ]
    }
   ],
   "source": [
    "#Adding transformations\n",
    "epochs = 5\n",
    "from keras.preprocessing import image\n",
    "train_transformed = vgg.get_batches(path+\"train\", batch_size=batch_size, gen=image.ImageDataGenerator(shear_range=0.1,rotation_range=5,vertical_flip=True,width_shift_range=0.02, height_shift_range=0.02 ))\n",
    "vgg.fit(train_transformed, val_batches, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 611s - loss: 0.1641 - acc: 0.9642 - val_loss: 0.0499 - val_acc: 0.9850\n"
     ]
    }
   ],
   "source": [
    "train_transformed = vgg.get_batches(path+\"train\", batch_size=batch_size, gen=image.ImageDataGenerator(rotation_range=5))\n",
    "vgg.fit(train_transformed, val_batches, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 613s - loss: 0.4193 - acc: 0.9169 - val_loss: 0.0855 - val_acc: 0.9805\n"
     ]
    }
   ],
   "source": [
    "train_transformed = vgg.get_batches(path+\"train\", batch_size=batch_size, gen=image.ImageDataGenerator(vertical_flip=True))\n",
    "vgg.fit(train_transformed, val_batches, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 611s - loss: 0.1130 - acc: 0.9745 - val_loss: 0.0831 - val_acc: 0.9790\n"
     ]
    }
   ],
   "source": [
    "train_transformed = vgg.get_batches(path+\"train\", batch_size=batch_size, gen=image.ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1))\n",
    "vgg.fit(train_transformed, val_batches, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches, predictions = vgg.test(path+'test', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fakeclass/9292.jpg', 'fakeclass/12026.jpg', 'fakeclass/9688.jpg', 'fakeclass/4392.jpg', 'fakeclass/779.jpg', 'fakeclass/2768.jpg', 'fakeclass/2399.jpg', 'fakeclass/12225.jpg', 'fakeclass/10947.jpg', 'fakeclass/1780.jpg']\n",
      "(12500, 2)\n"
     ]
    }
   ],
   "source": [
    "print(test_batches.filenames[0:10])\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to ./sub/train2_batchsize90_epochs5_all_transforms.csv\n",
      "DONE!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='./sub/train2_batchsize90_epochs5_all_transforms.csv' target='_blank'>./sub/train2_batchsize90_epochs5_all_transforms.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/nbs/ml/sub/train2_batchsize90_epochs5_all_transforms.csv"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Return image ID from its name\n",
    "def get_id(img_path):\n",
    "    fid = img_path.split(\"/\")[1].split(\".\")[0] #Get image ID\n",
    "    return fid\n",
    "\n",
    "ids = []\n",
    "dog_probs = []\n",
    "for img_path,pred in zip(test_batches.filenames, predictions):\n",
    "    fid = get_id(img_path)\n",
    "    dog_prob = pred[1]\n",
    "    ids.append(fid)\n",
    "    dog_probs.append(dog_prob)\n",
    "    \n",
    "df = pd.DataFrame({\"id\":ids, \"label\":dog_probs})\n",
    "#print(df)\n",
    "fn = \"./sub/train2_batchsize\" + str(batch_size) + \"_epochs\" + str(epochs) +\"_all_transforms.csv\"\n",
    "df.to_csv(fn, index=False)\n",
    "print(\"Write to \" + fn)\n",
    "print(\"DONE!\")\n",
    "from IPython.display import FileLink\n",
    "FileLink(fn)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Training History\n",
    "Train Session 1:\n",
    "1. Base line train: re-pull code from github, re-wrote the code to run the deep learning, used default batch_size = 64, epochs =1\n",
    "    Epoch 1/1\n",
    "23000/23000 [==============================] - 612s - loss: 0.1197 - acc: 0.9684 - val_loss: 0.0665 - val_acc: 0.9830\n",
    "Test score: 0.17580, filename: batchsize64_epochs1_base.csv\n",
    "2. Trained based on the base line model, added shear_range = 0.1\n",
    "Epoch 1/1\n",
    "23000/23000 [==============================] - 613s - loss: 0.1014 - acc: 0.9756 - val_loss: 0.0667 - val_acc: 0.9810\n",
    "Test score: 0.16436, filename: batchsize64_epochs1_base_shear.csv    \n",
    "3. base + shear + rotate (30)\n",
    "Epoch 1/1\n",
    "23000/23000 [==============================] - 611s - loss: 0.1641 - acc: 0.9642 - val_loss: 0.0499 - val_acc: 0.9850\n",
    "Test score: 0.17411, filename: batchsize64_epochs1_base_shear_rotate.csv  \n",
    "4. base + shear + rotate + vertical flip\n",
    "23000/23000 [==============================] - 613s - loss: 0.4193 - acc: 0.9169 - val_loss: 0.0855 - val_acc: 0.9805\n",
    "Test score: 0.16267, filename: batchsize64_epochs1_base_shear_rotate_flip.csv  \n",
    "5. base + shear + rotate + flip + shift (width+height:0.1)\n",
    "23000/23000 [==============================] - 611s - loss: 0.1130 - acc: 0.9745 - val_loss: 0.0831 - val_acc: 0.9790\n",
    "Test score: 0.17426, filename: batchsize64_epochs1_base_shear_rotate_flip_shift.csv  \n",
    "6. changed batch_size = 90, retrain the raw images based on the model from step 1-5\n",
    "23000/23000 [==============================] - 613s - loss: 0.0936 - acc: 0.9794 - val_loss: 0.0754 - val_acc: 0.9825\n",
    "Test score: 0.19362, filename: batchsize90_epochs1_base_shear_rotate_flip_shift.csv\n",
    "\n",
    "Train Session 2:        \n",
    "1. batch_size = 90, epochs = 1, start afresh\n",
    "23000/23000 [==============================] - 620s - loss: 0.1285 - acc: 0.9656 - val_loss: 0.0559 - val_acc: 0.9830\n",
    "Test score: 0.13399, filename: train2_batchsize90_epochs1.csv\n",
    "2. batch_size = 90, epochs = 5, added transformations       \n",
    "train_transformed = vgg.get_batches(path+\"train\", batch_size=batch_size, gen=image.ImageDataGenerator(shear_range=0.1,rotation_range=5,vertical_flip=True,width_shift_range=0.02, height_shift_range=0.02 ))\n",
    "\n",
    "Epoch 1/5\n",
    "23000/23000 [==============================] - 623s - loss: 0.3319 - acc: 0.9119 - val_loss: 0.0889 - val_acc: 0.9740\n",
    "Epoch 2/5\n",
    "23000/23000 [==============================] - 622s - loss: 0.2836 - acc: 0.9197 - val_loss: 0.0831 - val_acc: 0.9740\n",
    "Epoch 3/5\n",
    "23000/23000 [==============================] - 621s - loss: 0.2668 - acc: 0.9226 - val_loss: 0.0587 - val_acc: 0.9795\n",
    "Epoch 4/5\n",
    "23000/23000 [==============================] - 621s - loss: 0.2640 - acc: 0.9236 - val_loss: 0.0598 - val_acc: 0.9795\n",
    "Epoch 5/5\n",
    "23000/23000 [==============================] - 622s - loss: 0.2765 - acc: 0.9198 - val_loss: 0.0767 - val_acc: 0.9815\n",
    "In [80]:\n",
    "Test score: 0.14027, filename:train2_batchsize90_epochs5_all_transforms.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
