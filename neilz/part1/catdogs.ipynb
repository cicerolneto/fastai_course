{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division,print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils; reload(utils)\n",
    "from utils import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34manaconda2\u001b[0m/  \u001b[01;34mdata\u001b[0m/  \u001b[01;34mdownloads\u001b[0m/  \u001b[01;34mfastai_course\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls ../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path = \"data/dogscats/\"\n",
    "path = \"../../../data/dogscats/sample/\"\n",
    "result_path = \"../../../data/dogscats/results/\"\n",
    "# As large as you can, but no larger than 64 is recommended. \n",
    "# If you have an older or cheaper GPU, you'll run out of memory, so will have to decrease this.\n",
    "batch_size=60\n",
    "epochs = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 images belonging to 2 classes.\n",
      "Found 20 images belonging to 2 classes.\n",
      "{'cats': 0, 'dogs': 1}\n",
      "{'cats': 0, 'dogs': 1}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "vgg = Vgg16()\n",
    "# Grab a few images at a time for training and validation.\n",
    "# NB: They must be in subdirectories named based on their category\n",
    "batches = vgg.get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = vgg.get_batches(path+'valid', batch_size=batch_size*2)\n",
    "print(batches.class_indices)\n",
    "print(val_batches.class_indices)\n",
    "print(batches.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_20 (Lambda)           (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_236 (ZeroPadd (None, 3, 226, 226)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_235 (Conv2D)          (None, 64, 224, 224)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_237 (ZeroPadd (None, 64, 226, 226)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_236 (Conv2D)          (None, 64, 224, 224)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_91 (MaxPooling (None, 64, 112, 112)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_238 (ZeroPadd (None, 64, 114, 114)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_237 (Conv2D)          (None, 128, 112, 112)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_239 (ZeroPadd (None, 128, 114, 114)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_238 (Conv2D)          (None, 128, 112, 112)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_92 (MaxPooling (None, 128, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_240 (ZeroPadd (None, 128, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_239 (Conv2D)          (None, 256, 56, 56)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_241 (ZeroPadd (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_240 (Conv2D)          (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_242 (ZeroPadd (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_241 (Conv2D)          (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_93 (MaxPooling (None, 256, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_243 (ZeroPadd (None, 256, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_242 (Conv2D)          (None, 512, 28, 28)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_244 (ZeroPadd (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_243 (Conv2D)          (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_245 (ZeroPadd (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_244 (Conv2D)          (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_94 (MaxPooling (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_246 (ZeroPadd (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_245 (Conv2D)          (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_247 (ZeroPadd (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_246 (Conv2D)          (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_248 (ZeroPadd (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_247 (Conv2D)          (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_95 (MaxPooling (None, 512, 7, 7)         0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 8,194\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.finetune(batches)\n",
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/0 [==========================================================================================] - 2s 2s/step - loss: 0.9244 - acc: 0.6500 - val_loss: 0.6889 - val_acc: 0.7000\n",
      "Epoch 2/5\n",
      "1/0 [==========================================================================================] - 2s 2s/step - loss: 0.4215 - acc: 0.8000 - val_loss: 0.4663 - val_acc: 0.8500\n",
      "Epoch 3/5\n",
      "1/0 [==========================================================================================] - 2s 2s/step - loss: 0.1793 - acc: 0.9000 - val_loss: 0.3126 - val_acc: 0.9000\n",
      "Epoch 4/5\n",
      "1/0 [==========================================================================================] - 2s 2s/step - loss: 0.2006 - acc: 0.9000 - val_loss: 0.2350 - val_acc: 0.9500\n",
      "Epoch 5/5\n",
      "1/0 [==========================================================================================] - 2s 2s/step - loss: 0.0481 - acc: 1.0000 - val_loss: 0.1877 - val_acc: 0.9500\n"
     ]
    }
   ],
   "source": [
    "#vgg.finetune(batches)\n",
    "h = vgg.fit(batches, val_batches, nb_epoch=epochs, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'epoch', 'history', 'model', 'on_batch_begin', 'on_batch_end', 'on_epoch_begin', 'on_epoch_end', 'on_train_begin', 'on_train_end', 'params', 'set_model', 'set_params', 'validation_data']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.924385</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.688884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.421520</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.466296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.179260</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.312646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.200634</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.234958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.048054</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.187659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    acc      loss  val_acc  val_loss\n",
       "0  0.65  0.924385     0.70  0.688884\n",
       "1  0.80  0.421520     0.85  0.466296\n",
       "2  0.90  0.179260     0.90  0.312646\n",
       "3  0.90  0.200634     0.95  0.234958\n",
       "4  1.00  0.048054     0.95  0.187659"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dir(h))\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(h.history)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.model.save_weights(result_path + '/temp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_value(vgg.model.optimizer.lr, 0.01) # (default =0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beta_1': 0.8999999761581421,\n",
       " 'beta_2': 0.9990000128746033,\n",
       " 'decay': 0.0,\n",
       " 'epsilon': 1e-08,\n",
       " 'lr': 0.009999999776482582}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "23000/23000 [==============================] - 623s - loss: 0.3319 - acc: 0.9119 - val_loss: 0.0889 - val_acc: 0.9740\n",
      "Epoch 2/5\n",
      "23000/23000 [==============================] - 622s - loss: 0.2836 - acc: 0.9197 - val_loss: 0.0831 - val_acc: 0.9740\n",
      "Epoch 3/5\n",
      "23000/23000 [==============================] - 621s - loss: 0.2668 - acc: 0.9226 - val_loss: 0.0587 - val_acc: 0.9795\n",
      "Epoch 4/5\n",
      "23000/23000 [==============================] - 621s - loss: 0.2640 - acc: 0.9236 - val_loss: 0.0598 - val_acc: 0.9795\n",
      "Epoch 5/5\n",
      "23000/23000 [==============================] - 622s - loss: 0.2765 - acc: 0.9198 - val_loss: 0.0767 - val_acc: 0.9815\n"
     ]
    }
   ],
   "source": [
    "#Adding transformations\n",
    "epochs = 5\n",
    "from keras.preprocessing import image\n",
    "train_transformed = vgg.get_batches(path+\"train\", batch_size=batch_size, gen=image.ImageDataGenerator(shear_range=0.1,rotation_range=5,vertical_flip=True,width_shift_range=0.02, height_shift_range=0.02 ))\n",
    "vgg.fit(train_transformed, val_batches, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 611s - loss: 0.1641 - acc: 0.9642 - val_loss: 0.0499 - val_acc: 0.9850\n"
     ]
    }
   ],
   "source": [
    "train_transformed = vgg.get_batches(path+\"train\", batch_size=batch_size, gen=image.ImageDataGenerator(rotation_range=5))\n",
    "vgg.fit(train_transformed, val_batches, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 613s - loss: 0.4193 - acc: 0.9169 - val_loss: 0.0855 - val_acc: 0.9805\n"
     ]
    }
   ],
   "source": [
    "train_transformed = vgg.get_batches(path+\"train\", batch_size=batch_size, gen=image.ImageDataGenerator(vertical_flip=True))\n",
    "vgg.fit(train_transformed, val_batches, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 611s - loss: 0.1130 - acc: 0.9745 - val_loss: 0.0831 - val_acc: 0.9790\n"
     ]
    }
   ],
   "source": [
    "train_transformed = vgg.get_batches(path+\"train\", batch_size=batch_size, gen=image.ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1))\n",
    "vgg.fit(train_transformed, val_batches, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches, predictions = vgg.test(path+'test', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fakeclass/9292.jpg', 'fakeclass/12026.jpg', 'fakeclass/9688.jpg', 'fakeclass/4392.jpg', 'fakeclass/779.jpg', 'fakeclass/2768.jpg', 'fakeclass/2399.jpg', 'fakeclass/12225.jpg', 'fakeclass/10947.jpg', 'fakeclass/1780.jpg']\n",
      "(12500, 2)\n"
     ]
    }
   ],
   "source": [
    "#print(test_batches.filenames[0:10])\n",
    "#print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to ./sub/train4_batchsize90_epochs1_lr0.01.csv\n",
      "DONE!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='./sub/train4_batchsize90_epochs1_lr0.01.csv' target='_blank'>./sub/train4_batchsize90_epochs1_lr0.01.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/nbs/ml/sub/train4_batchsize90_epochs1_lr0.01.csv"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Return image ID from its name\n",
    "def get_id(img_path):\n",
    "    fid = img_path.split(\"/\")[1].split(\".\")[0] #Get image ID\n",
    "    return fid\n",
    "\n",
    "ids = []\n",
    "dog_probs = []\n",
    "for img_path,pred in zip(test_batches.filenames, predictions):\n",
    "    fid = get_id(img_path)\n",
    "    dog_prob = pred[1]\n",
    "    ids.append(fid)\n",
    "    dog_probs.append(dog_prob)\n",
    "    \n",
    "df = pd.DataFrame({\"id\":ids, \"label\":dog_probs})\n",
    "#print(df)\n",
    "fn = \"./sub/train4_batchsize\" + str(batch_size) + \"_epochs\" + str(epochs) +\"_lr0.01.csv\"\n",
    "df.to_csv(fn, index=False)\n",
    "print(\"Write to \" + fn)\n",
    "print(\"DONE!\")\n",
    "from IPython.display import FileLink\n",
    "FileLink(fn)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "image shape: \n",
      "(3, 256, 256)\n",
      "image dim_ordering: \n",
      "th\n",
      "Total number of images: 23000\n",
      "Processed 2000\n",
      "Processed 4000\n",
      "Processed 6000\n",
      "Processed 8000\n",
      "Processed 10000\n",
      "Processed 12000\n",
      "Processed 14000\n",
      "Processed 16000\n",
      "Processed 18000\n",
      "Processed 20000\n",
      "Processed 22000\n",
      "Completed generating zoom-ed images!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing import image\n",
    "\n",
    "dim_ordering = 'th'\n",
    "def mkdirp(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "def save_img(img_data, prefix, out_path, img_name, is_scale):\n",
    "    img = image.array_to_img(img_data, dim_ordering, scale=is_scale)\n",
    "    fname = '{prefix}_{img_name}'.format(prefix= prefix, img_name=img_name)\n",
    "    #print(out_path)\n",
    "    #print(fname)\n",
    "    img.save(os.path.join(out_path, fname))\n",
    "    \n",
    "transform = 'zoom'\n",
    "gen=image.ImageDataGenerator(zoom_range=0.2) #shear_range=5, horizontal_shift=True, rotation_range = 10, shift = 0.1\n",
    "\n",
    "base_path = \"data/dogscats/\"\n",
    "train_path = base_path + '/train'\n",
    "\n",
    "out_path = base_path + 'train_' + transform\n",
    "mkdirp(out_path)\n",
    "batches = gen.flow_from_directory(train_path, shuffle=False, batch_size=1)\n",
    "print('image shape: ')\n",
    "print(batches.image_shape)\n",
    "print('image dim_ordering: ')\n",
    "print(batches.dim_ordering)\n",
    "counts = len(batches.filenames)\n",
    "print('Total number of images: %d' %counts)\n",
    "for idx in range(len(batches.filenames)):\n",
    "    if(idx >= 2000 and idx %2000 == 0):\n",
    "        print('Processed %d'%idx)\n",
    "    img = batches.filenames[idx]\n",
    "    cls = img.split(\"/\")[0]\n",
    "    fn = img.split(\"/\")[1]\n",
    "    dat = batches.next()\n",
    "    outp = out_path + '/' + cls\n",
    "    mkdirp(outp)\n",
    "    #print(len(dat))\n",
    "    #print(dat[0].shape)\n",
    "    #print(dat[1].shape)\n",
    "    save_img(dat[0][0], transform, outp, fn, is_scale=True)\n",
    "    \n",
    "print('Completed generating %s-ed images!'%transform)    \n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training History\n",
    "Train Session 1:\n",
    "1. Base line train: re-pull code from github, re-wrote the code to run the deep learning, used default batch_size = 64, epochs =1\n",
    "    Epoch 1/1\n",
    "23000/23000 [==============================] - 612s - loss: 0.1197 - acc: 0.9684 - val_loss: 0.0665 - val_acc: 0.9830\n",
    "Test score: 0.17580, filename: batchsize64_epochs1_base.csv\n",
    "\n",
    "2. Trained based on the base line model, added shear_range = 0.1\n",
    "Epoch 1/1\n",
    "23000/23000 [==============================] - 613s - loss: 0.1014 - acc: 0.9756 - val_loss: 0.0667 - val_acc: 0.9810\n",
    "Test score: 0.16436, filename: batchsize64_epochs1_base_shear.csv    \n",
    "\n",
    "3. base + shear + rotate (30)\n",
    "Epoch 1/1\n",
    "23000/23000 [==============================] - 611s - loss: 0.1641 - acc: 0.9642 - val_loss: 0.0499 - val_acc: 0.9850\n",
    "Test score: 0.17411, filename: batchsize64_epochs1_base_shear_rotate.csv  \n",
    "\n",
    "4. base + shear + rotate + vertical flip\n",
    "23000/23000 [==============================] - 613s - loss: 0.4193 - acc: 0.9169 - val_loss: 0.0855 - val_acc: 0.9805\n",
    "Test score: 0.16267, filename: batchsize64_epochs1_base_shear_rotate_flip.csv  \n",
    "\n",
    "5. base + shear + rotate + flip + shift (width+height:0.1)\n",
    "23000/23000 [==============================] - 611s - loss: 0.1130 - acc: 0.9745 - val_loss: 0.0831 - val_acc: 0.9790\n",
    "Test score: 0.17426, filename: batchsize64_epochs1_base_shear_rotate_flip_shift.csv  \n",
    "\n",
    "6. changed batch_size = 90, retrain the raw images based on the model from step 1-5\n",
    "23000/23000 [==============================] - 613s - loss: 0.0936 - acc: 0.9794 - val_loss: 0.0754 - val_acc: 0.9825\n",
    "Test score: 0.19362, filename: batchsize90_epochs1_base_shear_rotate_flip_shift.csv\n",
    "\n",
    "Train Session 2:        \n",
    "1. batch_size = 90, epochs = 1, start afresh\n",
    "23000/23000 [==============================] - 620s - loss: 0.1285 - acc: 0.9656 - val_loss: 0.0559 - val_acc: 0.9830\n",
    "Test score: 0.13399, filename: train2_batchsize90_epochs1.csv\n",
    "\n",
    "2. batch_size = 90, epochs = 5, added transformations       \n",
    "train_transformed = vgg.get_batches(path+\"train\", batch_size=batch_size, gen=image.ImageDataGenerator(shear_range=0.1,rotation_range=5,vertical_flip=True,width_shift_range=0.02, height_shift_range=0.02 ))\n",
    "\n",
    "Epoch 1/5\n",
    "23000/23000 [==============================] - 623s - loss: 0.3319 - acc: 0.9119 - val_loss: 0.0889 - val_acc: 0.9740\n",
    "Epoch 2/5\n",
    "23000/23000 [==============================] - 622s - loss: 0.2836 - acc: 0.9197 - val_loss: 0.0831 - val_acc: 0.9740\n",
    "Epoch 3/5\n",
    "23000/23000 [==============================] - 621s - loss: 0.2668 - acc: 0.9226 - val_loss: 0.0587 - val_acc: 0.9795\n",
    "Epoch 4/5\n",
    "23000/23000 [==============================] - 621s - loss: 0.2640 - acc: 0.9236 - val_loss: 0.0598 - val_acc: 0.9795\n",
    "Epoch 5/5\n",
    "23000/23000 [==============================] - 622s - loss: 0.2765 - acc: 0.9198 - val_loss: 0.0767 - val_acc: 0.9815\n",
    "In [80]:\n",
    "Test score: 0.14027, filename:train2_batchsize90_epochs5_all_transforms.csv\n",
    "\n",
    "\n",
    "Train Session 3: Train on each transformation separately and ensemble (with base case) with average\n",
    "batch_size = 90, epochs = 1\n",
    "\n",
    "1. train_flip\n",
    "23000/23000 [==============================] - 800s - loss: 0.1288 - acc: 0.9647 - val_loss: 0.0525 - val_acc: 0.9840\n",
    "\n",
    "Test score: 0.13822, filename: train3_batchsize90_epochs1_flip.csv\n",
    "\n",
    "2. train_shift\n",
    "23000/23000 [==============================] - 801s - loss: 0.1409 - acc: 0.9608 - val_loss: 0.0813 - val_acc: 0.9775\n",
    "Test score: 0.16133, filename:train3_batchsize90_epochs1_shift.csv\n",
    "\n",
    "3. train_rotate\n",
    "23000/23000 [==============================] - 799s - loss: 0.1397 - acc: 0.9603 - val_loss: 0.0623 - val_acc: 0.9795\n",
    "Test score: 0.13268, filename: train3_batchsize90_epochs1_rotate.csv\n",
    "\n",
    "4. train_shear\n",
    "23000/23000 [==============================] - 798s - loss: 0.5209 - acc: 0.8065 - val_loss: 0.0681 - val_acc: 0.9780\n",
    "Test score: 0.11434, filename: train3_batchsize90_epochs1_shear.csv\n",
    "\n",
    "5. train_zoom\n",
    "23000/23000 [==============================] - 800s - loss: 0.1560 - acc: 0.9545 - val_loss: 0.0574 - val_acc: 0.9815\n",
    "Test score: 0.11701, filename: train3_batchsize90_epochs1_zoom.csv\n",
    "\n",
    "6. ensemble: average the predictions from all above\n",
    "Test score: 0.10128, filename: ensemble_session1_3.csv\n",
    "\n",
    "7. train_all: use raw and all transformed images for training\n",
    "138000/138000 [==============================] - 4357s - loss: 0.1812 - acc: 0.9448 - val_loss: 0.0671 - val_acc: 0.9790\n",
    "Test score: 0.15428, filename: train3_batchsize90_epochs1_all.csv\n",
    "\n",
    "8. make 2nd to last layer trainable\n",
    "23000/23000 [==============================] - 778s - loss: 0.1282 - acc: 0.9638 - val_loss: 0.0527 - val_acc: 0.9805\n",
    "Test score: 0.11644 filename: train3_batchsize90_epochs1_morelayer.csv\n",
    "\n",
    "9. make 2nd to last layer trainable + all data\n",
    "138000/138000 [==============================] - 4358s - loss: 0.1796 - acc: 0.9443 - val_loss: 0.0565 - val_acc: 0.9820\n",
    "\n",
    "Test score: 0.13980,  filename: train3_batchsize90_epochs1_morelayer_all.csv\n",
    "\n",
    "Train Session 4:\n",
    "1. train with raw images, batch_size = 90, epochs =1 once\n",
    "2. change learn rate to 0.01, train twice\n",
    "23000/23000 [==============================] - 783s - loss: 0.1242 - acc: 0.9680 - val_loss: 0.0552 - val_acc: 0.9845\n",
    "23000/23000 [==============================] - 783s - loss: 0.1215 - acc: 0.9670 - val_loss: 0.0462 - val_acc: 0.9870\n",
    "Test score: 0.13962, filename: train4_batchsize90_epochs1_lr0.01.csv\n",
    "with winsorization: [0.02, 0.98], I got test score = 0.07491."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
