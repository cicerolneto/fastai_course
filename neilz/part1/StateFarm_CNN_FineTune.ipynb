{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pwd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = '/home/ubuntu/data/statefarm/imags/' #imags/samples\n",
    "train_path = base_path + 'train'\n",
    "val_path = base_path + 'valid'\n",
    "test_path = base_path + 'test'\n",
    "weight_path = '/home/ubuntu/data/weights/statefarm/'\n",
    "sub_path = '/home/ubuntu/fastai_course/neilz/part1/sub/statefarm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'vgg16' from 'vgg16.pyc'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import vgg16; reload(vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Pre-Trained Convolution Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "mid_train_data = utils.load_array(weight_path + '/statefarm_conv_res')\n",
    "mid_val_data = utils.load_array(weight_path + '/statefarm_conv_val')\n",
    "mid_test_data = utils.load_array(weight_path + '/statefarm_conv_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "val_classes, train_classes, val_labels, train_labels, \\\n",
    "        val_filenames, train_filenames, test_filenames = utils.get_classes(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17943, 25088)\n"
     ]
    }
   ],
   "source": [
    "print(mid_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_3 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 6,526,730\n",
      "Trainable params: 6,476,042\n",
      "Non-trainable params: 50,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Construct a new model with just FCBs and using the convoluation results from Vgg16 as input\n",
    "#I want to overfit the train data first, so I use 2 dense layers total 4096*4096+4096 (bias) parameters without dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "lr = 0.001\n",
    "statefarm_model = Sequential()\n",
    "statefarm_model.add(BatchNormalization(axis=1, input_shape=(25088,)))\n",
    "statefarm_model.add(Dense(256, activation='relu')) #, input_shape=(25088,)))\n",
    "statefarm_model.add(BatchNormalization(axis=1))\n",
    "statefarm_model.add(Dropout(0.5)) \n",
    "statefarm_model.add(Dense(10, activation='softmax'))\n",
    "statefarm_model.compile(optimizer=Adam(lr=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "statefarm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beta_1': 0.8999999761581421, 'epsilon': 1e-08, 'beta_2': 0.9990000128746033, 'lr': 9.999999747378752e-05, 'decay': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.set_value(statefarm_model.optimizer.lr, 0.0001)\n",
    "print(statefarm_model.optimizer.get_config())\n",
    "statefarm_model.compile(optimizer=Adam(lr=lr), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17943 samples, validate on 4481 samples\n",
      "Epoch 1/10\n",
      "17943/17943 [==============================] - 3s 189us/step - loss: 0.2388 - acc: 0.9369 - val_loss: 0.0126 - val_acc: 0.9964\n",
      "Epoch 2/10\n",
      "17943/17943 [==============================] - 3s 181us/step - loss: 0.0114 - acc: 0.9985 - val_loss: 0.0063 - val_acc: 0.9975\n",
      "Epoch 3/10\n",
      "17943/17943 [==============================] - 3s 182us/step - loss: 0.0049 - acc: 0.9995 - val_loss: 0.0064 - val_acc: 0.9982\n",
      "Epoch 4/10\n",
      "17943/17943 [==============================] - 3s 178us/step - loss: 0.0027 - acc: 0.9999 - val_loss: 0.0048 - val_acc: 0.9984\n",
      "Epoch 5/10\n",
      "17943/17943 [==============================] - 3s 176us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.0043 - val_acc: 0.9989\n",
      "Epoch 6/10\n",
      "17943/17943 [==============================] - 3s 183us/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.0047 - val_acc: 0.9987\n",
      "Epoch 7/10\n",
      "17943/17943 [==============================] - 3s 181us/step - loss: 0.0014 - acc: 0.9999 - val_loss: 0.0045 - val_acc: 0.9987\n",
      "Epoch 8/10\n",
      "17943/17943 [==============================] - 3s 182us/step - loss: 8.2490e-04 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 0.9984\n",
      "Epoch 9/10\n",
      "17943/17943 [==============================] - 3s 178us/step - loss: 7.6066e-04 - acc: 0.9999 - val_loss: 0.0041 - val_acc: 0.9982\n",
      "Epoch 10/10\n",
      "17943/17943 [==============================] - 3s 178us/step - loss: 4.9348e-04 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 0.9980\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "#Need create train batch and validation batch\n",
    "batch_size = 256\n",
    "\n",
    "saved_weights_path = weight_path + '/fcn_2bns_1layers_dropout_epoch_{epoch:02d}-valloss_{val_loss:.2f}.hdf5'\n",
    "mcp = ModelCheckpoint(saved_weights_path, monitor='val_loss', save_weights_only=True, mode='auto', period=1)\n",
    "call_backs = [mcp]\n",
    "epochs = 10\n",
    "h = statefarm_model.fit(mid_train_data, train_labels, epochs=epochs, validation_data=(mid_val_data, val_labels), batch_size=batch_size, shuffle=True, callbacks=call_backs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/weights/statefarm//fcn_epoch_200-valloss_2.30.hdf5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'testm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1a43d6966c4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/fcn_epoch_200-valloss_2.30.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtestm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'testm' is not defined"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "mid_train_data = utils.load_array(weight_path + '/statefarm_conv_res')\n",
    "wp = weight_path + '/fcn_2bns_1layers_dropout_epoch_10-valloss_0.01.hdf5'\n",
    "print(wp)\n",
    "statefarm_model.load_weights(wp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Pre-Training Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAisAAAGHCAYAAABxmBIgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f88d98331d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "df = pd.DataFrame(h.history)\n",
    "df['epoch'] =df.index\n",
    "\n",
    "fig = plt.figure(1)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.set_ylim(0.98,1.02)\n",
    "ax.set_title('Accuracy vs. Epochs')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "line_train, =ax.plot(np.array(df['epoch']), np.array(df['acc']), '.', color='red', markersize=8)\n",
    "#plt.plot(x=np.array(df['epoch']), y=np.array(df['acc']), style='o', ylim=(0,1.1), markerfacecolor=\"None\", markeredgecolor='red')\n",
    "line_val, = ax.plot(np.array(df['epoch']), np.array(df['val_acc']), 'o',  markersize=4, markeredgecolor='blue')\n",
    "ax.plot(np.array(df['epoch']), np.ones(len(df)), 'black')\n",
    "ax.legend((line_train, line_val), ('Train Accuracy', 'Validation Accuracy'))\n",
    "fig.savefig(sub_path + '/train_accuracy_vs_epochs10_base.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1 BN, dropout\n",
    "preds_1bn_dropout = statefarm_model.predict(mid_test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2 2 BN, dropout\n",
    "preds_2bn_dropout = statefarm_model.predict(mid_test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 No BN, dropout 20 epochs\n",
    "preds_0bn_dropout1 = statefarm_model.predict(mid_test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4 No BN, dropout 40 epochs\n",
    "preds_0bn_dropout2 = statefarm_model.predict(mid_test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#5 No BN, dropout 100 epochs\n",
    "preds_0bn_dropout3 = statefarm_model.predict(mid_test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_labels = statefarm_model.predict(mid_test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combo_labels = np.concatenate([train_labels, test_labels])\n",
    "combo_data = np.concatenate([mid_train_data, mid_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97669, 25088)\n"
     ]
    }
   ],
   "source": [
    "print(combo_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_3 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 6,526,730\n",
      "Trainable params: 6,476,042\n",
      "Non-trainable params: 50,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Construct a new model with just FCBs and using the convoluation results from Vgg16 as input\n",
    "#I want to overfit the train data first, so I use 2 dense layers total 4096*4096+4096 (bias) parameters without dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "lr = 0.001\n",
    "statefarm_model_pseu = Sequential()\n",
    "statefarm_model_pseu.add(BatchNormalization(axis=1, input_shape=(25088,)))\n",
    "statefarm_model_pseu.add(Dense(256, activation='relu')) #, input_shape=(25088,)))\n",
    "statefarm_model_pseu.add(BatchNormalization(axis=1))\n",
    "statefarm_model_pseu.add(Dropout(0.5)) \n",
    "statefarm_model_pseu.add(Dense(10, activation='softmax'))\n",
    "statefarm_model_pseu.compile(optimizer=Adam(lr=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "statefarm_model_pseu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 97669 samples, validate on 4481 samples\n",
      "Epoch 1/10\n",
      "97669/97669 [==============================] - 16s 164us/step - loss: 0.7321 - acc: 0.8329 - val_loss: 0.0280 - val_acc: 0.9958\n",
      "Epoch 2/10\n",
      "97669/97669 [==============================] - 16s 163us/step - loss: 0.5554 - acc: 0.8893 - val_loss: 0.0222 - val_acc: 0.9962\n",
      "Epoch 3/10\n",
      "97669/97669 [==============================] - 16s 164us/step - loss: 0.5244 - acc: 0.9052 - val_loss: 0.0201 - val_acc: 0.9955\n",
      "Epoch 4/10\n",
      "97669/97669 [==============================] - 16s 163us/step - loss: 0.5103 - acc: 0.9129 - val_loss: 0.0192 - val_acc: 0.9971\n",
      "Epoch 5/10\n",
      "97669/97669 [==============================] - 16s 163us/step - loss: 0.5010 - acc: 0.9184 - val_loss: 0.0185 - val_acc: 0.9962\n",
      "Epoch 6/10\n",
      "97669/97669 [==============================] - 16s 163us/step - loss: 0.4958 - acc: 0.9218 - val_loss: 0.0188 - val_acc: 0.9960\n",
      "Epoch 7/10\n",
      "97669/97669 [==============================] - 16s 163us/step - loss: 0.4915 - acc: 0.9246 - val_loss: 0.0180 - val_acc: 0.9960\n",
      "Epoch 8/10\n",
      "97669/97669 [==============================] - 16s 164us/step - loss: 0.4872 - acc: 0.9275 - val_loss: 0.0183 - val_acc: 0.9960\n",
      "Epoch 9/10\n",
      "97669/97669 [==============================] - 16s 163us/step - loss: 0.4850 - acc: 0.9290 - val_loss: 0.0189 - val_acc: 0.9953\n",
      "Epoch 10/10\n",
      "97669/97669 [==============================] - 16s 163us/step - loss: 0.4832 - acc: 0.9291 - val_loss: 0.0197 - val_acc: 0.9951\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "#Need create train batch and validation batch\n",
    "batch_size = 256\n",
    "\n",
    "saved_weights_path = weight_path + '/pseudo_2bns_1layers_epoch_{epoch:02d}-valloss_{val_loss:.2f}.hdf5'\n",
    "mcp = ModelCheckpoint(saved_weights_path, monitor='val_loss', save_weights_only=True, mode='auto', period=1)\n",
    "call_backs = [mcp]\n",
    "epochs = 10\n",
    "h = statefarm_model_pseu.fit(combo_data, combo_labels, epochs=epochs, validation_data=(mid_val_data, val_labels), batch_size=batch_size, shuffle=True, callbacks=call_backs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = statefarm_model.predict(mid_test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Augmentated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def mkdirp(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "def save_img(img_data, prefix, out_path, img_name, is_scale):\n",
    "    dim_ordering = None # 'th'\n",
    "    img = image.array_to_img(img_data, dim_ordering, scale=is_scale)\n",
    "    fname = '{prefix}_{img_name}'.format(prefix= prefix, img_name=img_name)\n",
    "    #print(out_path)\n",
    "    #print(\"Transformed filename : %s\"%fname)\n",
    "    img.save(os.path.join(out_path, fname))\n",
    "    \n",
    "#gen= #shear_range=5, horizontal_shift=True, rotation_range = 10, shift = 0.1\n",
    "\n",
    "#base_path = train_path\n",
    "#train_path = base_path + '/train'\n",
    "\n",
    "\n",
    "def create_transformed_images(base_path, transform, gen):\n",
    "    print(\"Generating %s transforming images....\"%transform)\n",
    "    out_path = base_path + 'train_' + transform    \n",
    "    train_path = base_path + '/train'\n",
    "    mkdirp(out_path)\n",
    "    batches = gen.flow_from_directory(train_path, shuffle=False, batch_size=1)\n",
    "    #print('image shape: ')\n",
    "    #print(batches.image_shape)\n",
    "    #print('image dim_ordering: ')\n",
    "    counts = len(batches.filenames)\n",
    "    print('Total number of images: %d' %counts)\n",
    "    for idx in range(len(batches.filenames)):\n",
    "        if(idx >= 2000 and idx %2000 == 0):\n",
    "            print('Processed %d'%idx)\n",
    "        img = batches.filenames[idx]\n",
    "        cls = img.split(\"/\")[0]\n",
    "        fn = img.split(\"/\")[1]\n",
    "        dat = batches.next()\n",
    "        outp = out_path + '/' + cls\n",
    "        mkdirp(outp)\n",
    "        #print(len(dat))\n",
    "        #print(dat[0].shape)\n",
    "        #print(dat[1].shape)\n",
    "        save_img(dat[0][0], transform, outp, fn, is_scale=True)        \n",
    "    \n",
    "    print('Completed generating %s-ed images!'%transform)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17943 images belonging to 10 classes.\n",
      "Total number of images: 17943\n",
      "Processed 2000\n",
      "Processed 4000\n",
      "Processed 6000\n",
      "Processed 8000\n",
      "Processed 10000\n",
      "Processed 12000\n",
      "Processed 14000\n",
      "Processed 16000\n",
      "Completed generating hshift-ed images!\n",
      "Found 17943 images belonging to 10 classes.\n",
      "Total number of images: 17943\n",
      "Processed 2000\n",
      "Processed 4000\n",
      "Processed 6000\n",
      "Processed 8000\n",
      "Processed 10000\n",
      "Processed 12000\n",
      "Processed 14000\n",
      "Processed 16000\n",
      "Completed generating wshift-ed images!\n",
      "Found 17943 images belonging to 10 classes.\n",
      "Total number of images: 17943\n",
      "Processed 2000\n",
      "Processed 4000\n",
      "Processed 6000\n",
      "Processed 8000\n",
      "Processed 10000\n",
      "Processed 12000\n",
      "Processed 14000\n",
      "Processed 16000\n",
      "Completed generating zoom-ed images!\n",
      "Found 17943 images belonging to 10 classes.\n",
      "Total number of images: 17943\n",
      "Processed 2000\n",
      "Processed 4000\n",
      "Processed 6000\n",
      "Processed 8000\n",
      "Processed 10000\n",
      "Processed 12000\n",
      "Processed 14000\n",
      "Processed 16000\n",
      "Completed generating rotation-ed images!\n",
      "Found 17943 images belonging to 10 classes.\n",
      "Total number of images: 17943\n",
      "Processed 2000\n",
      "Processed 4000\n",
      "Processed 6000\n",
      "Processed 8000\n",
      "Processed 10000\n",
      "Processed 12000\n",
      "Processed 14000\n",
      "Processed 16000\n",
      "Completed generating zcawhitening-ed images!\n",
      "Found 17943 images belonging to 10 classes.\n",
      "Total number of images: 17943\n",
      "Processed 2000\n",
      "Processed 4000\n",
      "Processed 6000\n",
      "Processed 8000\n",
      "Processed 10000\n",
      "Processed 12000\n",
      "Processed 14000\n",
      "Processed 16000\n",
      "Completed generating shear-ed images!\n"
     ]
    }
   ],
   "source": [
    "generators = {'zoom':image.ImageDataGenerator(zoom_range=0.2),\n",
    "             'shear':image.ImageDataGenerator(shear_range=5),\n",
    "              'wshift':image.ImageDataGenerator(width_shift_range=0.1),\n",
    "              'hshift':image.ImageDataGenerator(height_shift_range=0.1),\n",
    "              'rotation':image.ImageDataGenerator(rotation_range=5),\n",
    "              'zcawhitening': image.ImageDataGenerator(zca_whitening=True)\n",
    "             }\n",
    "\n",
    "for transform, gen in generators.items():\n",
    "    create_transformed_images(base_path, transform, gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train CNN layers with VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vgg16.py:100: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  model.add(Conv2D(filters, 3, 3, activation='relu'))\n",
      "vgg16.py:100: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
      "  model.add(Conv2D(filters, 3, 3, activation='relu'))\n",
      "vgg16.py:100: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
      "  model.add(Conv2D(filters, 3, 3, activation='relu'))\n",
      "vgg16.py:100: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
      "  model.add(Conv2D(filters, 3, 3, activation='relu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 3, 226, 226)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 224, 224)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 64, 226, 226)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 224, 224)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 112, 112)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 64, 114, 114)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 128, 112, 112)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 128, 114, 114)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 112, 112)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 128, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 128, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 256, 56, 56)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 256, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 256, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 512, 28, 28)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPaddi (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 512, 7, 7)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import vgg16; reload(vgg16)\n",
    "vggm = vgg16.Vgg16()\n",
    "#Remove dense layers, only keep convolution layers\n",
    "vggm.model.pop() #dense6\n",
    "vggm.model.pop() #dropout4\n",
    "vggm.model.pop() #dense5\n",
    "vggm.model.pop() #dropout3\n",
    "vggm.model.pop() #dense4\n",
    "vggm.compile() \n",
    "vggm.model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing CNN output features for hshift\n",
      "Found 17943 images belonging to 10 classes.\n",
      "Computing CNN output features for wshift\n",
      "Found 17943 images belonging to 10 classes.\n",
      "Computing CNN output features for zoom\n",
      "Found 17943 images belonging to 10 classes.\n",
      "Computing CNN output features for rotation\n",
      "Found 17943 images belonging to 10 classes.\n",
      "Computing CNN output features for zcawhitening\n",
      "Found 17943 images belonging to 10 classes.\n",
      "Computing CNN output features for shear\n",
      "Found 17943 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 60\n",
    "for transform in generators.keys():\n",
    "    print(\"Computing CNN output features for %s\"%transform)\n",
    "    t_path = base_path +'/train_' + transform\n",
    "    t_batches, t_predictions = vggm.test(t_path, batch_size)\n",
    "    utils.save_array(weight_path + '/statefarm_' + transform, t_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17943, 25088)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Pre-Trained CNN to Augmented Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "transformed_train_data = None\n",
    "for transform in generators.keys():\n",
    "    weight_p = weight_path + '/statefarm_' + transform\n",
    "    t = utils.load_array(weight_p)\n",
    "    if transformed_train_data is None:\n",
    "        transformed_train_data = t\n",
    "    else:\n",
    "        transformed_train_data = np.concatenate([transformed_train_data, t])\n",
    "\n",
    "transformed_train_data = np.concatenate([transformed_train_data, mid_train_data])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125601, 25088)\n",
      "(125601, 10)\n"
     ]
    }
   ],
   "source": [
    "transformed_train_labels = np.concatenate([train_labels, train_labels,train_labels,train_labels,train_labels,train_labels, train_labels])\n",
    "print(transformed_train_data.shape)\n",
    "print(transformed_train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 6,526,730\n",
      "Trainable params: 6,476,042\n",
      "Non-trainable params: 50,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Construct a new model with just FCBs and using the convoluation results from Vgg16 as input\n",
    "#I want to overfit the train data first, so I use 2 dense layers total 4096*4096+4096 (bias) parameters without dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "lr = 0.001\n",
    "statefarm_model_pseu = Sequential()\n",
    "statefarm_model_pseu.add(BatchNormalization(axis=1, input_shape=(25088,)))\n",
    "statefarm_model_pseu.add(Dense(256, activation='relu')) #, input_shape=(25088,)))\n",
    "statefarm_model_pseu.add(BatchNormalization(axis=1))\n",
    "statefarm_model_pseu.add(Dropout(0.5)) \n",
    "statefarm_model_pseu.add(Dense(10, activation='softmax'))\n",
    "statefarm_model_pseu.compile(optimizer=Adam(lr=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "statefarm_model_pseu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 125601 samples, validate on 4481 samples\n",
      "Epoch 1/10\n",
      "125601/125601 [==============================] - 20s 161us/step - loss: 0.2897 - acc: 0.9121 - val_loss: 0.0090 - val_acc: 0.9975\n",
      "Epoch 2/10\n",
      "125601/125601 [==============================] - 20s 159us/step - loss: 0.1108 - acc: 0.9642 - val_loss: 0.0064 - val_acc: 0.9978\n",
      "Epoch 3/10\n",
      "125601/125601 [==============================] - 20s 159us/step - loss: 0.0766 - acc: 0.9744 - val_loss: 0.0061 - val_acc: 0.9984\n",
      "Epoch 4/10\n",
      "125601/125601 [==============================] - 20s 158us/step - loss: 0.0591 - acc: 0.9805 - val_loss: 0.0046 - val_acc: 0.9989\n",
      "Epoch 5/10\n",
      "125601/125601 [==============================] - 20s 158us/step - loss: 0.0451 - acc: 0.9854 - val_loss: 0.0058 - val_acc: 0.9984\n",
      "Epoch 6/10\n",
      "125601/125601 [==============================] - 20s 158us/step - loss: 0.0391 - acc: 0.9872 - val_loss: 0.0073 - val_acc: 0.9980\n",
      "Epoch 7/10\n",
      "125601/125601 [==============================] - 20s 158us/step - loss: 0.0328 - acc: 0.9892 - val_loss: 0.0070 - val_acc: 0.9973\n",
      "Epoch 8/10\n",
      "125601/125601 [==============================] - 20s 158us/step - loss: 0.0302 - acc: 0.9898 - val_loss: 0.0051 - val_acc: 0.9975\n",
      "Epoch 9/10\n",
      "125601/125601 [==============================] - 20s 157us/step - loss: 0.0269 - acc: 0.9912 - val_loss: 0.0054 - val_acc: 0.9984\n",
      "Epoch 10/10\n",
      "125601/125601 [==============================] - 20s 157us/step - loss: 0.0224 - acc: 0.9923 - val_loss: 0.0066 - val_acc: 0.9982\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "#Need create train batch and validation batch\n",
    "batch_size = 256\n",
    "\n",
    "saved_weights_path = weight_path + '/aug_2bns_1layers_epoch_{epoch:02d}-valloss_{val_loss:.2f}.hdf5'\n",
    "mcp = ModelCheckpoint(saved_weights_path, monitor='val_loss', save_weights_only=True, mode='auto', period=1)\n",
    "call_backs = [mcp]\n",
    "epochs = 10\n",
    "h = statefarm_model_pseu.fit(transformed_train_data, transformed_train_labels, epochs=epochs, validation_data=(mid_val_data, val_labels), batch_size=batch_size, shuffle=True, callbacks=call_backs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = statefarm_model_pseu.predict(mid_test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Augmented + Pseudo Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79726, 10)\n"
     ]
    }
   ],
   "source": [
    "print(test_labels.shape)\n",
    "combo_aug_pseudo_labels = np.concatenate([transformed_train_labels, test_labels])\n",
    "combo_aug_pseudo_data = np.concatenate([transformed_train_data, mid_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 205327 samples, validate on 4481 samples\n",
      "Epoch 1/10\n",
      "205327/205327 [==============================] - 32s 158us/step - loss: 0.3385 - acc: 0.9237 - val_loss: 0.0108 - val_acc: 0.9973\n",
      "Epoch 2/10\n",
      "205327/205327 [==============================] - 32s 157us/step - loss: 0.2777 - acc: 0.9433 - val_loss: 0.0094 - val_acc: 0.9975\n",
      "Epoch 3/10\n",
      "205327/205327 [==============================] - 32s 158us/step - loss: 0.2643 - acc: 0.9491 - val_loss: 0.0076 - val_acc: 0.9978\n",
      "Epoch 4/10\n",
      "205327/205327 [==============================] - 32s 158us/step - loss: 0.2582 - acc: 0.9522 - val_loss: 0.0079 - val_acc: 0.9975\n",
      "Epoch 5/10\n",
      "205327/205327 [==============================] - 33s 159us/step - loss: 0.2529 - acc: 0.9543 - val_loss: 0.0072 - val_acc: 0.9980\n",
      "Epoch 6/10\n",
      "205327/205327 [==============================] - 33s 158us/step - loss: 0.2484 - acc: 0.9568 - val_loss: 0.0068 - val_acc: 0.9982\n",
      "Epoch 7/10\n",
      "205327/205327 [==============================] - 37s 179us/step - loss: 0.2451 - acc: 0.9590 - val_loss: 0.0072 - val_acc: 0.9975\n",
      "Epoch 8/10\n",
      "205327/205327 [==============================] - 32s 158us/step - loss: 0.2416 - acc: 0.9601 - val_loss: 0.0073 - val_acc: 0.9978\n",
      "Epoch 9/10\n",
      "205327/205327 [==============================] - 32s 157us/step - loss: 0.2402 - acc: 0.9609 - val_loss: 0.0069 - val_acc: 0.9980\n",
      "Epoch 10/10\n",
      "205327/205327 [==============================] - 32s 157us/step - loss: 0.2368 - acc: 0.9633 - val_loss: 0.0074 - val_acc: 0.9980\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "#Need create train batch and validation batch\n",
    "batch_size = 256\n",
    "\n",
    "saved_weights_path = weight_path + '/aug_pseudo_2bns_1layers_epoch_{epoch:02d}-valloss_{val_loss:.2f}.hdf5'\n",
    "mcp = ModelCheckpoint(saved_weights_path, monitor='val_loss', save_weights_only=True, mode='auto', period=1)\n",
    "call_backs = [mcp]\n",
    "epochs = 10\n",
    "h = statefarm_model_pseu.fit(combo_aug_pseudo_data, combo_aug_pseudo_labels, epochs=epochs, validation_data=(mid_val_data, val_labels), batch_size=batch_size, shuffle=True, callbacks=call_backs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[[8 2 5]\n",
      " [2 5 6]]\n",
      "[[ 4.5  2.   4. ]\n",
      " [ 3.   5.   6. ]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3],[4,5,6]])\n",
    "b = np.array([[8,2,5],[2,5,6]])\n",
    "print(a)\n",
    "print(b)\n",
    "avg =(a + b)/2.0\n",
    "print(avg)\n",
    "\n",
    "predictions = (preds_1bn_dropout+preds_2bn_dropout+preds_0bn_dropout1+preds_0bn_dropout2+preds_0bn_dropout3)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wp = weight_path + '/'\n",
    "ws = ['aug_pseudo_2bns_1layers_epoch_10-valloss_0.01.hdf5', \n",
    "     'aug_2bns_1layers_epoch_10-valloss_0.01.hdf5', 'pseudo_2bns_1layers_epoch_10-valloss_0.02.hdf5',]\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "lr = 0.001\n",
    "testm = Sequential()\n",
    "testm.add(BatchNormalization(axis=1, input_shape=(25088,)))\n",
    "testm.add(Dense(256, activation='relu')) #, input_shape=(25088,)))\n",
    "testm.add(BatchNormalization(axis=1))\n",
    "testm.add(Dropout(0.5)) \n",
    "testm.add(Dense(10, activation='softmax'))\n",
    "testm.compile(optimizer=Adam(lr=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#testm.summary()\n",
    "\n",
    "predictions = None\n",
    "for w in ws:\n",
    "    testm.load_weights(wp + w)\n",
    "    preds = testm.predict(mid_test_data, batch_size = batch_size)\n",
    "    if predictions is None:\n",
    "        predictions = preds\n",
    "    else:\n",
    "        predictions = predictions + preds\n",
    "\n",
    "predictions = predictions/len(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = statefarm_model_pseu.predict(mid_test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to ./sub/statefarm/ensemble_aug_pseu_batchsize_256_epochs10_.csv\n",
      "DONE!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='./sub/statefarm/ensemble_aug_pseu_batchsize_256_epochs10_.csv' target='_blank'>./sub/statefarm/ensemble_aug_pseu_batchsize_256_epochs10_.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/fastai_course/neilz/part1/sub/statefarm/ensemble_aug_pseu_batchsize_256_epochs10_.csv"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def do_clip(arr, mx): \n",
    "    return np.clip(arr, (1-mx)/9, mx)\n",
    "\n",
    "def get_id(img_path):\n",
    "    fid = img_path.split(\"/\")[1] #Get image ID\n",
    "    return fid\n",
    "\n",
    "s = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'] #train_data.class_indices.keys()\n",
    "s.sort()\n",
    "df = pd.DataFrame(predictions, columns=s)\n",
    "for c in s:\n",
    "    df[c] = do_clip(df[c], 0.93)\n",
    "df['img'] = test_filenames #test_batches.filenames\n",
    "df['img'] = df.apply(lambda r: get_id(r['img']), axis=1)\n",
    "cols = cols = ['img'] + s\n",
    "df = df[cols]\n",
    "#print(df)\n",
    "fn = \"./sub/statefarm/ensemble_aug_pseu_batchsize_\" + str(batch_size) + \"_epochs\" + str(epochs) +\"_.csv\"\n",
    "df.to_csv(fn, index=False)\n",
    "print(\"Write to \" + fn)\n",
    "print(\"DONE!\")\n",
    "from IPython.display import FileLink\n",
    "FileLink(fn)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
